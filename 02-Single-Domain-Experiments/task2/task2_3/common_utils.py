"""
Task2_3 åŒè·¯å¾„è¾“å…¥å¯¹æ¯”å®éªŒå·¥å…·å‡½æ•°
æ”¯æŒåŸå§‹æ›²çº¿(data1) + å®Œæ•´ç‰¹å¾(data2)çš„åŒè·¯å¾„è¾“å…¥
"""
import pandas as pd
import numpy as np
import pickle
from pathlib import Path
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix
import warnings
warnings.filterwarnings('ignore')


def load_datasets():
    """åŠ è½½åŒè·¯å¾„æ•°æ®é›†"""
    print("ğŸ”„ åŠ è½½åŒè·¯å¾„æ•°æ®é›†...")
    
    base_dir = Path(__file__).parent
    datasets = {}
    
    # åŠ è½½åŸå§‹æ›²çº¿æ•°æ® (data1)
    print("ğŸ“Š åŠ è½½åŸå§‹æ›²çº¿æ•°æ® (data1)...")
    with open(base_dir / 'datasets/data1/crimp_force_curves_dataset_035.pkl', 'rb') as f:
        data1_035 = pickle.load(f)
    with open(base_dir / 'datasets/data1/crimp_force_curves_dataset_05.pkl', 'rb') as f:
        data1_05 = pickle.load(f)
    
    # åŠ è½½å®Œæ•´ç‰¹å¾æ•°æ® (data2)
    print("ğŸ”§ åŠ è½½å®Œæ•´ç‰¹å¾æ•°æ® (data2)...")
    with open(base_dir / 'datasets/data2/features_035.pkl', 'rb') as f:
        data2_035 = pickle.load(f)
    with open(base_dir / 'datasets/data2/features_05.pkl', 'rb') as f:
        data2_05 = pickle.load(f)
    
    datasets['data1'] = {'035': data1_035, '05': data1_05}
    datasets['data2'] = {'035': data2_035, '05': data2_05}
    
    print("âœ… æ•°æ®é›†åŠ è½½å®Œæˆ")
    return datasets


def align_and_prepare_dual_path(data1, data2):
    """å¯¹é½å’Œå‡†å¤‡åŒè·¯å¾„æ•°æ®"""
    print("ğŸ”— å¯¹é½åŒè·¯å¾„æ•°æ®...")
    
    # ä½¿ç”¨CrimpIDè¿›è¡Œåˆå¹¶
    merged_data = pd.merge(data1, data2, on='CrimpID', how='inner', suffixes=('_data1', '_data2'))
    
    # æå–åŸå§‹æ›²çº¿æ•°æ®
    raw_data = np.stack(merged_data['Force_curve_RoI'].values)
    
    # æå–ç‰¹å¾æ•°æ®ï¼ˆæ’é™¤æ ‡è¯†åˆ—ã€æ ‡ç­¾åˆ—å’ŒåŸå§‹æ›²çº¿åˆ—ï¼‰
    exclude_cols = [
        'CrimpID', 'Wire_cross-section_conductor_data1', 'Wire_cross-section_conductor_data2',
        'Force_curve_raw', 'Force_curve_baseline', 'Force_curve_RoI',
        'Main_label_string_data1', 'Main_label_string_data2', 
        'Sub_label_string_data1', 'Sub_label_string_data2',
        'Main-label_encoded_data1', 'Main-label_encoded_data2',
        'Sub_label_encoded_data1', 'Sub_label_encoded_data2', 
        'Binary_label_encoded_data1', 'Binary_label_encoded_data2',
        'CFM_label_encoded_data1', 'CFM_label_encoded_data2'
    ]
    
    feature_cols = [col for col in merged_data.columns if col not in exclude_cols]
    feat_data = merged_data[feature_cols].values
    
    # ä½¿ç”¨Sub_label_encodedä½œä¸º5ç±»æ•…éšœæ ‡ç­¾
    labels = merged_data['Sub_label_encoded_data1'].values
    
    print(f"âœ… æ•°æ®å¯¹é½å®Œæˆ: {len(raw_data)} æ ·æœ¬, åŸå§‹æ›²çº¿ç»´åº¦: {raw_data.shape[1]}, ç‰¹å¾ç»´åº¦: {feat_data.shape[1]}")
    
    return raw_data, feat_data, labels


def preprocess_dual_path_data(raw_data, feat_data, labels):
    """é¢„å¤„ç†åŒè·¯å¾„æ•°æ®"""
    print("ğŸ”§ é¢„å¤„ç†åŒè·¯å¾„æ•°æ®...")
    
    # æ ‡ç­¾ç¼–ç 
    label_encoder = LabelEncoder()
    y_encoded = label_encoder.fit_transform(labels)
    
    # æ•°æ®å½’ä¸€åŒ–
    raw_scaler = StandardScaler()
    feat_scaler = StandardScaler()
    
    raw_scaled = raw_scaler.fit_transform(raw_data)
    feat_scaled = feat_scaler.fit_transform(feat_data)
    
    # åˆå¹¶åŒè·¯å¾„æ•°æ®
    X_combined = np.concatenate([raw_scaled, feat_scaled], axis=1)
    
    print(f"âœ… é¢„å¤„ç†å®Œæˆ: åˆå¹¶ç‰¹å¾ç»´åº¦ {X_combined.shape[1]}, ç±»åˆ«æ•° {len(label_encoder.classes_)}")
    
    return X_combined, y_encoded, label_encoder, raw_scaler, feat_scaler


def split_train_test(X, y, test_size=0.2, random_state=42):
    """åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†"""
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=test_size, random_state=random_state, stratify=y
    )
    return X_train, X_test, y_train, y_test


def evaluate_predictions(y_true, y_pred):
    """è¯„ä¼°é¢„æµ‹ç»“æœ"""
    accuracy = accuracy_score(y_true, y_pred)
    precision = precision_score(y_true, y_pred, average='weighted', zero_division=0)
    recall = recall_score(y_true, y_pred, average='weighted', zero_division=0)
    f1 = f1_score(y_true, y_pred, average='weighted', zero_division=0)
    
    return {
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'f1_score': f1
    }


def save_results_to_csv(results, model_name, dataset_type, size_type, output_dir):
    """ä¿å­˜ç»“æœåˆ°CSVæ–‡ä»¶"""
    output_dir = Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # åˆ›å»ºç»“æœDataFrame
    df_results = pd.DataFrame([{
        'model': model_name,
        'dataset_type': dataset_type,
        'size_type': size_type,
        'accuracy': results['accuracy'],
        'precision': results['precision'],
        'recall': results['recall'],
        'f1_score': results['f1_score']
    }])
    
    # ä¿å­˜åˆ°CSV
    filename = f"{model_name}_{dataset_type}_{size_type}.csv"
    filepath = output_dir / filename
    df_results.to_csv(filepath, index=False)
    
    print(f"ğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {filepath}")
    return filepath


def create_summary_csv(output_dir):
    """åˆ›å»ºæ±‡æ€»CSVæ–‡ä»¶"""
    output_dir = Path(output_dir)
    csv_files = list(output_dir.glob("*.csv"))
    
    if not csv_files:
        print("âš ï¸ æ²¡æœ‰æ‰¾åˆ°CSVæ–‡ä»¶")
        return
    
    # åˆå¹¶æ‰€æœ‰ç»“æœ
    all_results = []
    for csv_file in csv_files:
        df = pd.read_csv(csv_file)
        all_results.append(df)
    
    summary_df = pd.concat(all_results, ignore_index=True)
    summary_file = output_dir / "summary.csv"
    summary_df.to_csv(summary_file, index=False)
    
    print(f"ğŸ“Š æ±‡æ€»ç»“æœå·²ä¿å­˜åˆ°: {summary_file}")
    return summary_file
